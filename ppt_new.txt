import os
import time
import pandas as pd
from pptx import Presentation
import textract
import pptx2txt2
import warnings
warnings.filterwarnings("ignore")

# ----------------------------- CONFIG ---------------------------------
PPT_FOLDER = "pptx_files"     # Folder containing all pptx files
OUTPUT_EXCEL = "pptx_extraction_comparison.xlsx"
CHECK_READING_ORDER = True    # Toggle reading order check
BASELINE_LIB = "python-pptx"  # Used for accuracy comparison
# ----------------------------------------------------------------------

# ----------------------------- EXTRACTORS ------------------------------

def extract_text_python_pptx(file_path):
    prs = Presentation(file_path)
    text = []
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text.append(shape.text)
            elif shape.has_table:
                for row in shape.table.rows:
                    for cell in row.cells:
                        text.append(cell.text)
    return "\n".join(text)

def extract_text_textract(file_path):
    """Extract text using textract"""
    return textract.process(file_path).decode("utf-8", errors="ignore")

def extract_text_pptx2txt2(file_path):
    """Extract text using pptx2txt2"""
    return pptx2txt2.process(file_path)

# ----------------------------------------------------------------------
# ----------------------------- UTILITIES -------------------------------

def check_reading_order(text):
    """Simple heuristic for reading order detection"""
    if not text or len(text.strip()) < 10:
        return "Insufficient data"
    lines = text.strip().split("\n")
    avg_len = sum(len(l.strip()) for l in lines) / len(lines)
    if avg_len < 10:
        return "Line-wise"
    elif avg_len < 50:
        return "Paragraph-wise"
    else:
        return "Random"

def text_metrics(text):
    """Return text length and word count"""
    if not text:
        return {"length": 0, "word_count": 0}
    words = text.split()
    return {"length": len(text), "word_count": len(words)}

def calculate_accuracy(base_text, compare_text):
    """Approximate text overlap accuracy"""
    if not base_text or not compare_text:
        return 0
    base_words = set(base_text.split())
    comp_words = set(compare_text.split())
    if not base_words:
        return 0
    return round(len(base_words & comp_words) / len(base_words), 3)

def completeness_score(base_text, compare_text):
    """Ratio of extracted words vs baseline words"""
    if not base_text:
        return 0
    base_len = len(base_text.split())
    comp_len = len(compare_text.split())
    return round(min(comp_len / base_len, 1), 3) if base_len > 0 else 0

def noise_ratio(text):
    """Estimate noise ratio based on special chars"""
    if not text:
        return 0
    specials = sum(1 for c in text if not (c.isalnum() or c.isspace()))
    return round(specials / len(text), 3)

def duplicate_rate(text):
    """Estimate duplicate line ratio"""
    if not text:
        return 0
    lines = [l.strip() for l in text.split("\n") if l.strip()]
    if not lines:
        return 0
    unique = len(set(lines))
    return round(1 - (unique / len(lines)), 3)

def quality_score(acc, comp, noise, dup):
    """Weighted quality score"""
    return round((acc * 0.4 + comp * 0.3 + (1 - noise) * 0.2 + (1 - dup) * 0.1), 3)

# ----------------------------------------------------------------------
# ---------------------------- MAIN PROCESS -----------------------------

extractors = {
    "python-pptx": extract_text_python_pptx,
    "textract": extract_text_textract,
    "pptx2txt2": extract_text_pptx2txt2
}

results_summary = []
errors = []
os.makedirs(PPT_FOLDER, exist_ok=True)
ppt_files = [f for f in os.listdir(PPT_FOLDER) if f.endswith(".pptx")]

extracted_texts = {lib: {} for lib in extractors.keys()}

# ------------------------ Extraction Loop ------------------------------
for lib_name, func in extractors.items():
    print(f"Processing with: {lib_name}")
    lib_results = []

    for ppt_file in ppt_files:
        ppt_path = os.path.join(PPT_FOLDER, ppt_file)
        start_time = time.time()

        try:
            text = func(ppt_path)
            time_taken = round(time.time() - start_time, 2)
            extracted_texts[lib_name][ppt_file] = text

            metrics = text_metrics(text)
            order = check_reading_order(text) if CHECK_READING_ORDER else "Skipped"

            lib_results.append({
                "File": ppt_file,
                "Time (s)": time_taken,
                "Text Length": metrics["length"],
                "Word Count": metrics["word_count"],
                "Reading Order": order,
                "Accuracy": None,
                "Completeness": None,
                "Noise Ratio": noise_ratio(text),
                "Duplicate Rate": duplicate_rate(text),
                "Quality Score": None
            })

        except Exception as e:
            errors.append({"Library": lib_name, "File": ppt_file, "Error": str(e)})
            lib_results.append({
                "File": ppt_file,
                "Time (s)": None,
                "Text Length": None,
                "Word Count": None,
                "Reading Order": "Error",
                "Accuracy": None,
                "Completeness": None,
                "Noise Ratio": None,
                "Duplicate Rate": None,
                "Quality Score": None
            })

    results_summary.append((lib_name, pd.DataFrame(lib_results)))

# ----------------------------------------------------------------------
# ----------------------- Compute Accuracy & Quality --------------------

if BASELINE_LIB in extracted_texts:
    base_dict = extracted_texts[BASELINE_LIB]
    for lib_name, df in results_summary:
        if lib_name == BASELINE_LIB:
            for i, row in df.iterrows():
                df.loc[i, "Accuracy"] = 1.0
                df.loc[i, "Completeness"] = 1.0
                df.loc[i, "Quality Score"] = 1.0
            continue

        for i, row in df.iterrows():
            file_name = row["File"]
            base_text = base_dict.get(file_name, "")
            comp_text = extracted_texts[lib_name].get(file_name, "")
            acc = calculate_accuracy(base_text, comp_text)
            comp = completeness_score(base_text, comp_text)
            noise = row["Noise Ratio"]
            dup = row["Duplicate Rate"]
            qscore = quality_score(acc, comp, noise, dup)

            df.loc[i, "Accuracy"] = acc
            df.loc[i, "Completeness"] = comp
            df.loc[i, "Quality Score"] = qscore

# ----------------------------------------------------------------------
# ----------------------- Write to Excel Output -------------------------

with pd.ExcelWriter(OUTPUT_EXCEL, engine="openpyxl") as writer:
    combined_summary_rows = []

    for lib_name, df in results_summary:
        # Metrics sheet
        df.to_excel(writer, sheet_name=f"{lib_name}_Metrics", index=False)

        # Extracted text sheet
        text_data = pd.DataFrame(
            [(file, extracted_texts[lib_name].get(file, "")) for file in ppt_files],
            columns=["File", "Extracted Text"]
        )
        text_data.to_excel(writer, sheet_name=f"{lib_name}_Text", index=False)

        for _, r in df.iterrows():
            combined_summary_rows.append({
                "Library": lib_name,
                **r.to_dict()
            })

    pd.DataFrame(combined_summary_rows).to_excel(writer, sheet_name="Combined_Summary", index=False)
    pd.DataFrame(errors).to_excel(writer, sheet_name="Errors", index=False)

print(f"\n✅ Extraction completed successfully.\nResults saved to {OUTPUT_EXCEL}")

------------------------------

Great — below is a clear, thorough justification and description for every metric and piece of logic in your PPTX extraction benchmark. I explain why each metric matters, how it’s computed in the code, what its values mean, important caveats, and recommended actions when a metric is bad. Use this as a reference to interpret the Excel report or to tweak weights/thresholds.


---

Overall design principle (short)

We compare extractors not just by “how much text they return” but by usefulness of that text for downstream tasks (search/indexing, NLP, human reading). That requires measuring multiple orthogonal properties: correctness (accuracy), completeness, cleanliness (noise/duplicates), layout fidelity (reading order), speed, and robustness (errors). Combining them yields a single Quality Score while preserving the individual metrics for diagnosis.


---

Metric-by-metric: definition, rationale, formula, caveats, thresholds & actions

1) Time (s)

What: Time (in seconds) taken by the extractor to process one PPT file.
Why: Efficiency matters for production — throughput and cost. A slightly slower extractor may be acceptable if quality is much better, but not vice-versa.
How computed: end_time - start_time measured around the extraction call.
Caveats: Includes library startup overhead (Tika/LibreOffice may incur JVM / process startup cost). For fair comparisons, run repeated trials or keep long-running server versions.
Interpretation (example thresholds):

Very Fast: < 0.5s (small slides)

Acceptable: 0.5–2s

Slow: > 2s (investigate) Action if bad: Consider batching, running server-mode (Tika), or using a faster library for indexing while using the accurate one for critical tasks.



---

2) Text Length (characters)

What: Total number of characters in the extracted text.
Why: Quick signal of volume — very short outputs often indicate missed text, truncated files, or failed extraction.
How computed: len(extracted_text) in code.
Caveats: More characters is not always better (may include junk or repeated content). Use together with Noise Ratio and Duplicate Rate.
Interpretation:

If length ≈ baseline → likely good.

If much less → missing content.

If much more → possible duplication or garbage.
Action if bad: Inspect sample extracted text; check for repeated blocks or binary artifacts.



---

3) Word Count

What: Number of words in extracted text.
Why: Less sensitive to punctuation noise than character length; aligns better with semantic content.
How computed: len(text.split()).
Caveats: Splitting logic is naive (might break on punctuation). Prefer word counts for coarse completeness checks.
Action if mismatch with baseline: check where words are missing — headers, footers, notes, table cells.


---

4) Accuracy Score (Overlap with baseline)

What: Proportion of baseline words found in the extractor’s output. Formula used in code:
Accuracy = |A ∩ B| / |A| where A = set(words from baseline extractor, e.g., python-pptx), B = set(words from tested extractor.
Why: Measures how much of the authoritative extraction is preserved; baseline should be a fairly reliable extractor (we used python-pptx as baseline because it reads native PPTX structure).
How computed: Convert both texts to token sets, compute intersection size / baseline size. (In code we use simple whitespace tokenization.)
Caveats:

Set-based comparison ignores duplicates and ordering.

Word-tokenization differences (case, punctuation) affect result — normalization (lowercasing, punctuation removal) helps.

Baseline choice matters — if baseline misses something, accuracy is measured relative to that baseline. Consider an ensemble baseline (merge outputs) for higher confidence. Interpretation:

1.0 (100%): Extractor contains all baseline words (good).

< 0.8: significant differences — investigate missing content.
Action if low: Inspect which slide elements are not captured (tables, notes, grouped shapes), then pick extractor that preserves those elements.



---

5) Completeness Score

What: Ratio of extracted word count to baseline word count, capped at 1.0.
Why: Measures quantity coverage relative to baseline (are we extracting all blocks?).
How computed: min(len(B)/len(A), 1) where A = baseline word count, B = tested extractor.
Caveats: If baseline contains duplicates or noise, completeness may be misleading. Use with Accuracy.
Interpretation:

1.0 → equal or more words than baseline (may indicate duplication)

<1.0 → missing content
Action if low: check which elements are skipped (hidden shapes, notes) and whether that matters for your use-case.



---

6) Reading Order

What: A categorical label indicating how natural the reading sequence is in the extracted text: e.g. Best / Good / Bad (or Line-wise / Paragraph-wise / Random) depending on the heuristic.
Why: For document search and human-readable output, preserving visual reading order matters. If text flows logically, contextual extraction (QA, summarization) will work better.
How computed: Two strategies in the code:

If coordinates are available (Python-pptx shapes or PyMuPDF blocks), compare the order returned by the extractor to the natural top→left order (Spearman correlation / LCS).

If coordinates are not available, use heuristics on line lengths, punctuation continuity, or changes between consecutive lines (jumps) to infer disorder.
Caveats:

Heuristics are fallible. Two-column layouts may appear “mixed” even if output is semantically fine.

Some downstream models (bag-of-words, embeddings) are order-insensitive, so reading order may be lower priority.
Interpretation & Actions:

Best/Line-wise → no action.

Good → acceptable for search, but check multi-column slides.

Bad/Random → consider alternate extractor or postprocessing (re-order by y,x if coordinates available).



---

7) Structure Type

What: A high-level label that describes the structural nature of the slide text: Plain, Bulleted, Tabular, Mixed.
Why: Different extractors excel at different structures (e.g., tables vs bullet lists). Knowing slide structure helps interpret other metrics and decide extractor trade-offs.
How computed: Heuristic detection — count bullet symbols, count lines containing | (or tab separators) and decide.
Caveats: Heuristics miss fancy table formatting or SmartArt; detecting SmartArt requires deeper shape parsing.
Action if mismatch: If many slides are Tabular but extractor handles tables poorly, prefer Python-pptx or a table-aware extractor.


---

8) Noise Ratio

What: Fraction of characters that look like noise / non-alphanumeric / non-whitespace.
Why: High noise indicates encoding issues, OCR garbage, or HTML/markup leakage. Clean text is essential for downstream NLP.
How computed: weird_chars / total_chars where weird_chars = count of chars with ord outside typical printable ASCII range (or other heuristics).
Caveats: Non-Latin scripts increase this measure if you only expect ASCII. For multilingual documents, refine “weird” detection to allowed Unicode ranges.
Interpretation:

~0 → clean text

>0.05 → suspicious — inspect samples
Action if high: check for encoding problems, use appropriate decoding, or try a different extractor (Tika/OCR fallback).



---

9) Duplicate Rate

What: Proportion of duplicate lines among all non-empty lines: 1 - unique_lines / total_lines.
Why: Repetition may indicate that an extractor redundantly reads the same shape multiple times (common with flattened XML or layout confusion). Duplicates inflate length and mislead indexing.
How computed: Build a set of distinct lines and compare to total.
Caveats: Some legitimate slides may repeat labels (e.g., footer on every slide) — consider deduplicating footer lines before final scoring.
Interpretation:

0 → no duplication

>0.1 → moderate duplication

>0.3 → severe duplication
Action if high: add deduplication step or fix extractor settings.



---

10) Multilingual Support

What: Detects presence of non-English scripts (simple heuristic for Indic ranges in code).
Why: Many extractors fail on non-Latin characters; verifying multilingual text is crucial for correct indexing.
How computed: Check for characters in target Unicode ranges (e.g., Bengali, Devanagari).
Caveats: This is a binary / heuristic check; extend to detect specific languages with language detection libraries if needed.
Action if unsupported: choose extractors with robust Unicode handling or add language-aware postprocessing.


---

11) Quality Score (composite)

What: Weighted composite that summarizes overall extraction quality into one score for easier ranking. Example weights used in the code:
Quality = 0.4*Accuracy + 0.3*Completeness + 0.2*(1 - Noise) + 0.1*(1 - Duplication)
Why: Single-number ranking is convenient for initial selection, but you must look at individual metrics before making a final decision.
How computed: Weighted linear combination of normalized metrics (all between 0 and 1).
Caveats: Weights are subjective — adjust to reflect your priorities (e.g., for table-heavy slides increase completeness/table weight).
Interpretation: Higher is better (max 1.0). Use it for quick ranking but inspect all component metrics for the top pick.


---

12) Errors (Robustness)

What: Count and detail of failures when running extraction (exceptions, crashes).
Why: A library that gives perfect text but crashes frequently is useless in production.
How computed: Log exception messages, stack traces, and whether extraction returned empty or partially processed results.
Action: Prefer stable extractors with low error rates. For occasional failing files, consider fallbacks (e.g., if python-pptx fails, use pdf conversion + PyMuPDF).


---

Practical interpretation guidance (how to compare libraries)

1. Start with Quality Score to rank candidates. Then inspect top 2–3 libraries in detail.


2. Look at Completeness vs Accuracy:

High completeness + low accuracy → many extra words that aren't in baseline (maybe noise/duplication).

Low completeness + high accuracy → crisp but partial extraction (misses text blocks).



3. If tables matter: prefer extractors that show low Noise, high Accuracy on table lines, and evidence of table extraction (serialized table cells).


4. If layout matters (two-column slides, labels): reading order and block-coordinates matter — prefer extractors exposing coordinates (python-pptx, PyMuPDF).


5. If multilingual: check multilingual support and low noise for those scripts.


6. If at scale: balance Time with Quality Score. Consider server-mode or hybrid approach (fast extractor for search, slower accurate extractor for detailed views).




---

Example decision rules (recommendations)

If Quality Score >= 0.9 and Time < 1s → default pick for production.

If Quality Score >= 0.95 but Time > 2s → use as authoritative extractor for indexing periodically, but use a faster library for near-real-time ingest.

If Completeness < 0.8 → reject for datasets where missing content is critical.

If Noise Ratio > 0.05 or Duplication Rate > 0.2 → avoid unless you implement aggressive cleaning.



---

Final notes & best practices

Normalize tokens before similarity checks (lowercase, remove punctuation) if you want more robust Accuracy.

Use a small human-reviewed ground-truth set (10–50 typical PPTs) for final evaluation rather than relying solely on a single baseline.

Tune weights in the Quality Score to your business needs (e.g., legal compliance may prioritize completeness).

Log extracted text per-file and per-extractor (already in the Excel) — nothing replaces spot-checking raw outputs for edge cases (SmartArt, grouped shapes, OCR text inside images).

Consider a hybrid system: use python-pptx or other structural extractors for tables and shapes, and Tika/textract for fallback and the rest.
