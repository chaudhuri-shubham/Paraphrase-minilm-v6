Subject: Caf√© r√©sum√© meeting ‚òï
Body:

Hola Shubham,

Let's meet at the caf√© near the station tomorrow.
¬øPodr√≠as traer el r√©sum√© y los documentos de Mar√≠a L√≥pez, por favor?
Also, confirm for Se√±or Garc√≠a.

Thanks üòä


# demo_shiftjis_error.py
# This reproduces the exact same 'shift_jis' UnicodeDecodeError

# Make text containing accented characters that produce 0xF3 etc. in Latin-1
latin_text = "Se√±or L√≥pez met at the caf√©. C√≥mo est√°s?"

# Encode it as Latin-1 bytes (produces 0xF3 for '√≥', 0xF1 for '√±')
latin_bytes = latin_text.encode("latin-1")

# Try decoding those bytes as Shift_JIS (what extract_msg might do incorrectly)
decoded = latin_bytes.decode("shift_jis")


# fake_msg_error.py

latin_text = "Hola Se√±or, c√≥mo est√°s en el caf√©?"
data = latin_text.encode("latin-1")  # contains 0xF3

with open("broken.msg", "wb") as f:
    f.write(data)



with open("broken.msg", "rb") as f:
    raw = f.read()

raw.decode("shift_jis")  # ‚ùå Raises the same UnicodeDecodeError


import extract_msg

# Create a dummy .msg file (not a real Outlook one)
with open("broken.msg", "wb") as f:
    f.write("Se√±or L√≥pez en el caf√©".encode("latin-1"))

# Patch extract_msg to force Shift_JIS decode
msg = extract_msg.Message("broken.msg")

try:
    # Emulate what happens when wrong codec is used
    raw = msg._getStringStream("__substg1.0_1000")
    text = raw.decode("shift_jis")  # this line forces the same error
except UnicodeDecodeError as e:
    print("üí• Got expected error:", e)

....................

import extract_msg

msg = extract_msg.Message("accent_test.msg")

raw = msg._getStringStream("__substg1.0_1000")  # raw message body bytes
print("Raw bytes sample:", raw[:40])

# Force Shift_JIS decode on bytes that aren't Shift_JIS
raw.decode("shift_jis")  # üí• should trigger UnicodeDecodeError

.....................

import extract_msg

msg = extract_msg.Message("accent_test.msg")
raw = msg._getStringStream("__substg1.0_1000")

# Check for problematic Latin-1 bytes
illegal_bytes = [b for b in raw if b in (0xF1, 0xF3, 0xE9, 0xFA)]
print("Problem bytes found:", [hex(b) for b in illegal_bytes])

............

This is a test üí• ‚Äî contains emoji and non-SJIS characters like ƒÖ, ≈Ç, ƒá, ‰Ω†Â•Ω



..............

# Step 3: Simulate extract_msg‚Äôs internal decoding
bad_bytes = "„ÉÜ„Çπ„Éà".encode("shift_jis") + b"\xf0\x9f\x92\xa5"  # invalid emoji bytes

# Try decoding with shift_jis
try:
    text = bad_bytes.decode("shift_jis")
except UnicodeDecodeError as e:
    print("üí• Got the error!")
    print(e)

_____________________

# invalid shift_jis bytes
invalid_bytes = b'\x82\xa0\x82\xa2\x82\xa4\xf3\xf0\x9f\x92\xa5'  
# first few bytes (0x82a0 etc.) are valid Japanese in Shift_JIS
# last few (0xf3, 0xf0, 0x9f, 0x92, 0xa5) simulate emoji UTF-8 data

from olefile import OleFileIO, isOleFile
import struct

# Create a blank OLE2 file
with open("broken_shiftjis.msg", "wb") as f:
    # Minimal header for OLE structured file (we‚Äôll fake most of it)
    f.write(b"\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1")  # magic bytes for OLE2
    f.write(b"\x00" * 512)  # pad rest of header (we‚Äôre not building a valid FAT)

import olefile

# Open existing .msg
src = "template.msg"
dst = "broken_shiftjis.msg"

# Open OLE file
ole = olefile.OleFileIO(src)

# Copy to new file
import shutil
shutil.copy(src, dst)

# Reopen for modification (we‚Äôll hex-edit the stream)
with open(dst, "r+b") as f:
    data = f.read()
    # Find the 'Hello' bytes or any body-like string and replace
    new_data = data.replace(b"Hello", invalid_bytes)
    f.seek(0)
    f.write(new_data)

import extract_msg

msg = extract_msg.Message("broken_shiftjis.msg")
print(msg.body)


................


import boto3
import pandas as pd

# --- Create the S3 client ---
s3 = boto3.client(
    's3',
    endpoint_url='https://vpce-xxxxxxxxxxxxx.s3.ap-south-1.vpce.amazonaws.com',
    verify='/path/to/your/cabundle.pem'
)

bucket_name = 'your-bucket-name'

# --- Initialize pagination ---
paginator = s3.get_paginator('list_objects_v2')
page_iterator = paginator.paginate(Bucket=bucket_name)

# --- Collect all objects ---
all_files = []
for page in page_iterator:
    if 'Contents' in page:
        for obj in page['Contents']:
            all_files.append({
                'Key': obj['Key'],
                'LastModified': obj['LastModified'],
                'Size (Bytes)': obj['Size'],
                'StorageClass': obj.get('StorageClass', ''),
                'ETag': obj.get('ETag', '')
            })

# --- Create DataFrame ---
df = pd.DataFrame(all_files)

print(f"Total files fetched: {len(df)}")
df.head(10)